{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from datetime import date\n",
    "from ci_mapping.data.mag_orm import Paper, Author, AuthorAffiliation, Affiliation, AffiliationLocation, PaperAuthor, FieldOfStudy, PaperFieldsOfStudy, Conference, Journal\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lists(l):\n",
    "    \"\"\"Unpacks nested lists into one list of elements.\n",
    "\n",
    "    Args:\n",
    "        lst (:obj:`list` of :obj:`list`)\n",
    "\n",
    "    Returns\n",
    "        (list)\n",
    "    \n",
    "    \"\"\"\n",
    "    return [item for sublist in l if sublist for item in sublist if item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file and create a session.\n",
    "db_config = 'postgres+psycopg2://postgres@localhost/ai_ci'\n",
    "engine = create_engine(db_config)\n",
    "Session = sessionmaker(engine)\n",
    "s = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MAG data\n",
    "mag = pd.read_sql(s.query(Paper).statement, s.bind)\n",
    "print(f'MAG data shape: {mag.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling\n",
    "Mainly changing `string` to `np.nan` and codes from Microsoft Academic to human-readable labels.\n",
    "\n",
    "### Data decisions\n",
    "* Dropping papers published in 2020 (found 16 instances). The analysis will focus on full years.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 2020 papers\n",
    "mag = mag[mag.year!='2020']\n",
    "\n",
    "# Some columns have null values registered as 'NaN'\n",
    "mag['bibtex_doc_type'] = mag.bibtex_doc_type.replace('NaN', np.nan)\n",
    "mag['publisher'] = mag.publisher.replace('NaN', np.nan)\n",
    "mag['references'] = mag.references.replace('NaN', np.nan)\n",
    "mag['inverted_abstract'] = mag.inverted_abstract.replace('NaN', np.nan)\n",
    "mag['doi'] = mag.doi.replace('NaN', np.nan)\n",
    "\n",
    "# String to list\n",
    "mag['references'] = mag.references.apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else np.nan)\n",
    "\n",
    "# Change the publication and the bibtex document types\n",
    "publication_type_ = {'0':np.nan, \n",
    "                     '1':'Journal article', \n",
    "                     '2':'Patent', \n",
    "                     '3':'Conference paper',\n",
    "                     '4':'Book chapter',\n",
    "                     '5':'Book',\n",
    "                     '6':'Book reference entry', \n",
    "                     '7':'Dataset', \n",
    "                     '8':'Repository'}\n",
    "\n",
    "bibtext_doc_type_ = {'a':'Journal article', 'b':'Book', 'c':'Book chapter', 'p':'Conference paper'}\n",
    "\n",
    "mag['publication_type'] = mag.publication_type.apply(lambda x: publication_type_[x])\n",
    "mag['bibtex_doc_type'] = mag.bibtex_doc_type.apply(lambda x: bibtext_doc_type_[x] if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read journals\n",
    "journal = pd.read_sql(s.query(Journal).statement, s.bind)\n",
    "print(f'Journals data shape: {journal.shape}')\n",
    "\n",
    "# Read conferences\n",
    "conferences = pd.read_sql(s.query(Conference).statement, s.bind)\n",
    "print(f'Conferences data shape: {conferences.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CI papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41% of the DOIs, 22% of the abstracts and 39% of the references are missing. When looking only at papers with a DOI, 13% of the abstracts and 20.95% of the references are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of missing values\n",
    "(mag.isnull().sum() / mag.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicated titles. Investigate this once online\n",
    "mag.title.value_counts()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absence of a DOI doesn't seem to massively affect the results. Nevertheless, it skews them a bit to the right. As the graph below shows, paper publication peaks in 2015 (left). However, when examining publications with a DOI, paper publication peaks in 2017 (right). \n",
    "\n",
    "In both graphs, there's a rapid increase in paper publication from 2007 till 2015/7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "mag[mag.year > '1970'].groupby('year').count()['id'].plot(ax=ax1)\n",
    "ax1.set_title('CI papers in a year')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "# ax2\n",
    "mag[mag.year > '1970'][~mag[mag.year > '1970'].doi.isnull()].groupby('year').count()['id'].plot(ax=ax2)\n",
    "ax2.set_title('CI papers in a year (with DOI)')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "ax2.set_xlabel('Year')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the publication types, bibtext document types and publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "mag.publication_type.value_counts().plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Publication types')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "\n",
    "# ax2\n",
    "mag.bibtex_doc_type.value_counts().plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Bibtext document types')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "\n",
    "# ax3\n",
    "mag.publisher.value_counts()[:5].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('Publisher')\n",
    "ax3.set_ylabel('Raw frequency')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count how the number of publishers and publication types has changed over time.\n",
    "\n",
    "The data gathered from Microsoft Academic contain few patents and none of them has a DOI (first row of the graph below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "mag.groupby(['year', 'publication_type']).count()['id'].unstack('publication_type').plot(kind='bar', stacked=True, ax=ax1)\n",
    "ax1.set_title('CI papers by publication type')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "# ax2\n",
    "mag[~mag.doi.isnull()].groupby(['year', 'publication_type']).count()['id'].unstack('publication_type').plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('CI papers by publication type (with DOI)')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "ax2.set_xlabel('Year')\n",
    "\n",
    "# Find the publishers with most papers on disinfo\n",
    "top_publishers = mag.publisher.value_counts()[:8].index\n",
    "\n",
    "# ax3\n",
    "pub = mag[mag.publisher.isin(top_publishers)].groupby(['year', 'publisher']).count()['id']\n",
    "pub.unstack('publisher').plot(kind='bar', stacked=True, ax=ax3)\n",
    "ax3.set_title('CI papers by publisher (publication type)')\n",
    "ax3.set_ylabel('Raw frequency')\n",
    "ax3.set_xlabel('Year')\n",
    "\n",
    "# ax4\n",
    "pub = mag[(~mag.doi.isnull()) & (mag.publisher.isin(top_publishers))].groupby(['year', 'publisher']).count()['id']\n",
    "pub.unstack('publisher').plot(kind='bar', stacked=True, ax=ax4)\n",
    "ax4.set_title('CI papers by publisher (with DOI)')\n",
    "ax4.set_ylabel('Raw frequency')\n",
    "ax4.set_xlabel('Year')\n",
    "\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge journalsl & conferences with publication year\n",
    "journal = journal.merge(mag[['id', 'year']], left_on='paper_id', right_on='id')\n",
    "conferences = conferences.merge(mag[['id', 'year']], left_on='paper_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_journals = journal.journal_name.value_counts()[:10].index\n",
    "top_conferences = conferences.conference_name.value_counts()[:10].index\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,10))\n",
    "journal[journal.journal_name.isin(top_journals)].groupby(['year', 'journal_name'])['paper_id'].count().unstack('journal_name').plot(kind='bar', stacked=True, ax=ax1)\n",
    "ax1.set_title('CI papers by journal')\n",
    "ax1.set_ylabel('Raw frequency')\n",
    "ax1.set_xlabel('Year')\n",
    "\n",
    "conferences[conferences.conference_name.isin(top_conferences)].groupby(['year', 'conference_name'])['paper_id'].count().unstack('conference_name').plot(kind='bar', stacked=True, ax=ax2)\n",
    "ax2.set_title('CI papers by conference')\n",
    "ax2.set_ylabel('Raw frequency')\n",
    "ax2.set_xlabel('Year')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers with the most citations\n",
    "\n",
    "We find the average number of citations a paper has received through its lifetime:\n",
    "\n",
    "```math\n",
    "CitationsCount / (CurrentYear - PublicationYear)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(nrows=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "citations = mag.sort_values('citations', ascending=False).head(20)[['citations', 'year']]\n",
    "citations['year'] = citations.year.apply(lambda x: int(x))\n",
    "citations.plot(kind='scatter', x='year', y='citations', rot=90, ax=ax1)\n",
    "ax1.set_title('Paper citations')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticks([int(i) for i in sorted(mag.year.unique())])\n",
    "ax1.set_xticklabels(sorted(mag.year.unique()))\n",
    "\n",
    "\n",
    "# ax1\n",
    "mag['avg_citations'] = [row['citations'] / (2020 - int(row['year'])) for _, row in mag.iterrows()]\n",
    "avg_citations = mag.sort_values('avg_citations', ascending=False).head(20)[['avg_citations', 'year']]\n",
    "avg_citations['year'] = avg_citations.year.apply(lambda x: int(x))\n",
    "avg_citations.plot(kind='scatter', x='year', y='avg_citations', rot=90, ax=ax2)\n",
    "ax2.set_title('Paper average citations')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('Year')\n",
    "ax2.set_xticks([int(i) for i in sorted(mag.year.unique())])\n",
    "ax2.set_xticklabels(sorted(mag.year.unique()))\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_refs = set(flatten_lists(mag.references.dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"% of CI papers in references: {(mag.id.unique().shape[0] / len(unique_refs)) * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter([paper for paper in flatten_lists(mag.references.dropna()) if paper not in mag.id.unique()]).most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = pd.read_sql(s.query(Author).statement, s.bind)\n",
    "paper_authors = pd.read_sql(s.query(PaperAuthor).statement, s.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average number of co-authors in CI papers: {paper_authors.groupby('paper_id').count()['author_id'].mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank authors by the number of papers they have published"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_paper_count = pd.DataFrame(paper_authors.groupby('author_id').count()['paper_id']).reset_index()\n",
    "author_paper_count = author_paper_count.rename(index=str, columns={'author_id':'id', 'paper_id':'paper_count'})\n",
    "author_names_with_paper_count = authors.merge(author_paper_count, left_on='id', right_on='id').sort_values('paper_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(7,5))\n",
    "author_names_with_paper_count[['name', 'paper_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax)\n",
    "ax.set_title('Author rank: Paper count')\n",
    "ax.set_ylabel('Name')\n",
    "ax.set_xlabel('Count')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank authors by the raw frequency and the average number of times they have been cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge paper_authors with mag to get their citation and avg_citations count\n",
    "paper_authors_citations = paper_authors.merge(mag[['avg_citations', 'citations', 'id']], left_on='paper_id', right_on='id')\n",
    "\n",
    "# Raw citation count\n",
    "author_citations_sum = paper_authors_citations.groupby('author_id')['citations'].sum().reset_index()\n",
    "author_citations_sum = author_citations_sum.rename(index=str, columns={'author_id':'id', 'citations':'citation_count'})\n",
    "author_names_with_citation_count = authors.merge(author_citations_sum, left_on='id', right_on='id').sort_values('citation_count', ascending=False)\n",
    "\n",
    "# Average citation count\n",
    "author_avg_citations_sum = paper_authors_citations.groupby('author_id')['avg_citations'].sum().reset_index()\n",
    "author_avg_citations_sum = author_avg_citations_sum.rename(index=str, columns={'author_id':'id', 'avg_citations':'avg_citation_count'})\n",
    "author_names_with_avg_citation_count = authors.merge(author_avg_citations_sum, left_on='id', right_on='id').sort_values('avg_citation_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,6))\n",
    "\n",
    "# ax1\n",
    "author_names_with_avg_citation_count[['name', 'avg_citation_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax1)\n",
    "ax1.set_title('Author rank: Average citation count')\n",
    "ax1.set_ylabel('Name')\n",
    "ax1.set_xlabel('Count')\n",
    "\n",
    "# ax2\n",
    "author_names_with_citation_count[['name', 'citation_count']].head(10).set_index('name').plot(kind='barh', legend=False, ax=ax2)\n",
    "ax2.set_title('Author rank: Citation count')\n",
    "ax2.set_ylabel('Name')\n",
    "ax2.set_xlabel('Count')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Affiliations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affiliations =  pd.read_sql(s.query(Affiliation).statement, s.bind)\n",
    "author_aff =  pd.read_sql(s.query(AuthorAffiliation).statement, s.bind)\n",
    "location =  pd.read_sql(s.query(AffiliationLocation).statement, s.bind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some authors have multiple affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_aff.groupby('author_id').count()['affiliation_id'].sort_values(ascending=False)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Unique author affiliations: {affiliations.shape[0]}')\n",
    "print(f'% of geocoded affiliations: {(location.shape[0] / affiliations.shape[0]) * 100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of missing values\n",
    "(location.isnull().sum() / location.shape[0]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all tables\n",
    "df = paper_authors.merge(author_aff, left_on='author_id', right_on='author_id') \\\n",
    "                  .merge(location, left_on='affiliation_id', right_on='affiliation_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate authors\n",
    "df = df.drop_duplicates('author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(ncols=2, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "df.groupby('country')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Country')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2 \n",
    "df.groupby('name')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Affiliation')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax3\n",
    "df.groupby('administrative_area_level_2')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('City')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "# ax4\n",
    "df.groupby('administrative_area_level_1')['author_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Region')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "f.suptitle('Geography of CI research', y=1.02)\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the publication year\n",
    "df = df.merge(mag[['id', 'year', 'citations']], left_on='paper_id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,8))\n",
    "\n",
    "# ax1\n",
    "df.groupby('name')['paper_id'].count().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Institutions with the most papers in CI')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# ax2\n",
    "df.groupby('name')['citations'].sum().sort_values(ascending=False)[:25].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('Institutions with the most citations in CI')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fields of study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fos = pd.read_sql(s.query(FieldOfStudy).statement, s.bind)\n",
    "pfos = pd.read_sql(s.query(PaperFieldsOfStudy).statement, s.bind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of unique Fields of Study in CI: {fos.id.unique().shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add field of study names, year and doi to papers\n",
    "pfos = pfos.merge(fos, left_on='field_of_study_id', right_on='id')[['paper_id', 'field_of_study_id', 'name', 'id']]\n",
    "pfos = pfos.merge(mag[['id', 'doi', 'year']], left_on='paper_id', right_on='id')\n",
    "\n",
    "# Keep only papers with doi\n",
    "pfos_doi = pfos.dropna(subset=['doi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "pfos.name.value_counts()[:25].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('Most used Fields of Study')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('Fields of Study')\n",
    "\n",
    "# ax2\n",
    "i = pd.IndexSlice\n",
    "query_terms = [\"Crowdsourcing\", \"Citizen science\", \"Collective intelligence\", \"Wisdom of crowds\", \"Collective wisdom\", \n",
    "               \"Wisdom of the crowd\", \"Social computing\", \"Human computation\", \"Collaborative learning\"]\n",
    "pfos.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax2)\n",
    "ax2.set_title('Trending CI terms')\n",
    "\n",
    "# ax3\n",
    "i = pd.IndexSlice\n",
    "query_terms = ['Knowledge management', 'Psychology', 'Multimedia', 'Educational technology', \n",
    "               'Cooperative learning', 'Artificial intelligence', 'Machine learning', 'Active learning', 'Pedagogy']\n",
    "pfos.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax3)\n",
    "ax3.set_title('Trending CI fields')\n",
    "\n",
    "# ax4\n",
    "pfos_doi.name.value_counts()[:25].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Most used Fields of Study (only with DOI)')\n",
    "ax4.set_ylabel('Count')\n",
    "ax4.set_xlabel('Fields of Study')\n",
    "\n",
    "# ax5\n",
    "i = pd.IndexSlice\n",
    "query_terms = [\"Crowdsourcing\", \"Citizen science\", \"Collective intelligence\", \"Wisdom of crowds\", \"Collective wisdom\", \n",
    "               \"Wisdom of the crowd\", \"Social computing\", \"Human computation\", \"Collaborative learning\"]\n",
    "pfos_doi.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax5)\n",
    "ax5.set_title('Trending CI terms (only with DOI)')\n",
    "\n",
    "# ax6\n",
    "i = pd.IndexSlice\n",
    "query_terms = ['Knowledge management', 'Psychology', 'Multimedia', 'Educational technology', \n",
    "               'Cooperative learning', 'Artificial intelligence', 'Machine learning', 'Active learning', 'Pedagogy']\n",
    "pfos_doi.groupby(['year', 'name'])['paper_id'].count().loc[i[:, query_terms]].unstack('name').plot(ax=ax6)\n",
    "ax6.set_title('Trending CI fields (only with DOI)')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country level differences - What are the most used FoS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfos_doi_geo = pfos_doi.merge(df[['paper_id', 'country']], left_on='paper_id', right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfos_doi_geo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
    "\n",
    "# ax1\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['United States'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax1)\n",
    "ax1.set_title('United States')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['United Kingdom'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax2)\n",
    "ax2.set_title('United Kingdom')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax3\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Canada'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax3)\n",
    "ax3.set_title('Canada')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "# ax4\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Australia'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax4)\n",
    "ax4.set_title('Australia')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "# ax5\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['China'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax5)\n",
    "ax5.set_title('China')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "# ax6\n",
    "pfos_doi_geo.groupby(['country', 'name'])['paper_id'].count().loc['Spain'].sort_values(ascending=False)[:10].plot(kind='bar', ax=ax6)\n",
    "ax6.set_title('Spain')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How has Computer science, Artificial intelligence and Machine learning been used through time in the countries with the most CI papers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(nrows=3, figsize=(12,8))\n",
    "\n",
    "# ax1\n",
    "pfos_doi_geo[(pfos_doi_geo.name=='Computer science') & (pfos_doi_geo.year > '2000')].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:6].values].plot(kind='bar', rot=0, ax=ax1)\n",
    "ax1.set_title('FoS: Computer science')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# ax2\n",
    "pfos_doi_geo[(pfos_doi_geo.name=='Artificial intelligence') & (pfos_doi_geo.year > '2000')].groupby(['year', 'country'])['paper_id'].count().unstack('country')[df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:6].values].plot(kind='bar', rot=0, ax=ax2)\n",
    "ax2.set_title('FoS: Artificial intelligence')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "# ax3\n",
    "pfos_doi_geo[(pfos_doi_geo.name=='Machine learning') & (pfos_doi_geo.year > '2000')].groupby(['year', 'country'])['paper_id'].count().unstack('country')[[c for c in df.groupby('country')['author_id'].count().sort_values(ascending=False).index[:6].values if c != 'Brazil']].plot(kind='bar', rot=0, ax=ax3)\n",
    "ax3.set_title('FoS: Machine learning')\n",
    "ax3.set_ylabel('Count')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A network of CI research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_graph(elements):\n",
    "    # Get all of the unique entries you have\n",
    "    varnames = tuple(sorted(set(itertools.chain(*elements))))\n",
    "\n",
    "    # Get a list of all of the combinations you have\n",
    "    expanded = [tuple(itertools.combinations(d, 2)) for d in elements]\n",
    "    expanded = itertools.chain(*expanded)\n",
    "\n",
    "    # Sort the combinations so that A,B and B,A are treated the same\n",
    "    expanded = [tuple(sorted(d)) for d in expanded]\n",
    "\n",
    "    # count the combinations\n",
    "    return Counter(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(pfos.groupby('paper_id')['name'].apply(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for k, v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 20:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/ci_mapping/data/interim/ci_fos.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition the network\n",
    "np.random.seed(42)\n",
    "communities = community.best_partition(G, resolution=.61)\n",
    "print(f'Number of communities: {len(set(communities.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colours = {0:'#7e1e9c', 1:'#15b01a', 2:'#0343df', 3:'#f97306', 4:'#e50000', 5:'#ffff14'}\n",
    "colours = {0:u'#1f77b4', 1:u'#ff7f0e', 2:u'#2ca02c', 3:u'#d62728', 4:u'#9467bd', \n",
    "           5:u'#8c564b', 6:u'#e377c2', 7:u'#7f7f7f', 8:u'#bcbd22', 9:u'#17becf'}\n",
    "node_list = list(G.nodes())\n",
    "colour_dict = {k:colours[communities[k]] for k in node_list}\n",
    "nx.set_node_attributes(G, colour_dict, 'color')\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/ci_mapping/data/interim/ci_fos_coloured_v2.graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import spacy\n",
    "from itertools import chain\n",
    "from spacy_langdetect import LanguageDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted2abstract(obj):\n",
    "    if isinstance(obj, str):\n",
    "        inverted_index = json.loads(obj)['InvertedIndex']\n",
    "        d = {}\n",
    "\n",
    "        for k, v in inverted_index.items():\n",
    "            if len(v)==1:\n",
    "                d[v[0]] = k\n",
    "            else:\n",
    "                for idx in v:\n",
    "                    d[idx] = k\n",
    "        \n",
    "        return ' '.join([v for _, v in OrderedDict(sorted(d.items())).items()])\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mag['abstract'] = mag.inverted_abstract.apply(inverted2abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en\n",
    "# !python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "nlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n",
    "# ner_labels = ['GPE', 'NORP', 'PERSON', 'ORG', 'LOC', 'FAC', ]\n",
    "ner_labels = ['CARDINAL', 'ORDINAL', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'DATE']\n",
    "\n",
    "def name_entities(text, ner_labels):\n",
    "    doc = nlp(text)\n",
    "    if doc._.language['language'] == 'en':\n",
    "        return [tuple((ent.text, ent.label_)) for ent in doc.ents if ent.label_ not in ner_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "d = {}\n",
    "for _, row in mag.dropna(subset=['abstract']).iterrows():\n",
    "    d[row['id']] = name_entities(row['abstract'], ner_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = flatten_lists(list(d.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "entity = []\n",
    "entity_type = []\n",
    "for k, v in d.items():\n",
    "    if v:\n",
    "        for item in v:\n",
    "            ids.append(k)\n",
    "            entity.append(v[0][0])\n",
    "            entity_type.append(v[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common entities in abstracts\n",
    "Counter(ents).most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities = pd.DataFrame({'id':ids, 'entity':entity, 'entity_type':entity_type})\n",
    "# abstract_entities = abstract_entities.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ents = abstract_entities.merge(df[['country', 'paper_id', 'year']], left_on='id', right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_ents[papers_ents.entity=='Chinese'].groupby('country').count()['paper_id'].sort_values(ascending=False)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_entities.entity.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords\n",
    "import inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kw = {}\n",
    "for idx, text in mag.abstract.dropna().iteritems():\n",
    "    if nlp(text)._.language['language'] == 'en':\n",
    "        kw[idx] = keywords.keywords(text, split=True, ratio=.2)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(kw.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lookups import Lookups\n",
    "lookups = Lookups()\n",
    "lookups.add_table(\"lemma_rules\", {\"noun\": [[\"s\", \"\"]]})\n",
    "lemmatizer = Lemmatizer(lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = []\n",
    "for tokens in words:\n",
    "    lst = []\n",
    "    for token in tokens:\n",
    "        lst.extend(lemmatizer(token, 'NOUN'))\n",
    "    w.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(words)\n",
    "\n",
    "G = nx.Graph()\n",
    "for k,v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 7:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_textrank.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cooccurrence network of fields of study\n",
    "graph = cooccurrence_graph(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "for k,v in graph.items():\n",
    "    # Keep only edges where the pair has cooccurred more than 5 times\n",
    "    if v > 8:\n",
    "        G.add_edge(k[0], k[1], weight=int(v))\n",
    "    \n",
    "print(f'Nodes: {len(G)}')\n",
    "print(f'Edges: {len(G.edges)}')\n",
    "\n",
    "nx.write_graphml(G, path='/Users/kstathou/Desktop/fnf/data/interim/disinfo_textrank_singular_words.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
